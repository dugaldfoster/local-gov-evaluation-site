---
title: "Methods"
description: "A practical menu of rigorous approaches — from implementation learning to causal impact and value for money."
image: "/img/methods.jpg"
---

## Three core types of evaluation

Most local government evaluations fall into one (or more) of these three types. Strong evaluations are explicit about which they are doing.

### 1. Process evaluation  
**Purpose:** Understand how a programme was implemented, for whom, and why it worked (or didn’t).  
**Best when:** Piloting new services, improving delivery, scaling up interventions.

**Typical questions**
- Was the intervention delivered as intended?
- Who engaged, who didn’t, and why?
- What barriers and enablers affected delivery?
- How did staff and residents experience the service?

**Rigorous methods commonly used**
- Theory of Change development and testing
- Structured implementation logs (delivery fidelity, reach, dosage)
- Qualitative interviews / focus groups (staff and residents)
- Observation of practice (e.g. frontline delivery)
- Routine monitoring data with clear denominators
- Framework analysis or realist-informed analysis (Context–Mechanism–Outcome)

**Example (local government)**
> Evaluating a new housing officer outreach model:  
> - Develop Theory of Change with officers and tenants  
> - Track weekly caseload, visit types, and follow-up actions  
> - Interview tenants who disengaged as well as those who engaged  
> - Identify which elements are feasible to scale and which are not  

---

### 2. Impact evaluation  
**Purpose:** Estimate whether the intervention caused change in outcomes.  
**Best when:** You need to justify investment, scale up, or defend decisions.

Impact evaluations vary in strength depending on design.

#### Weaker designs (use with caution)
- Before/after only  
  → Vulnerable to external trends (e.g. cost of living, seasonal effects)

#### Stronger quasi-experimental designs
These are often feasible using routine council or partner datasets:

- **Matched comparison groups**  
  - Match participants to similar non-participants (e.g. using age, deprivation, prior service use)
- **Difference-in-Differences (DiD)**  
  - Compare change over time between intervention group and comparator group
- **Interrupted time series**  
  - Use long trends in outcomes (e.g. monthly homelessness presentations) before and after implementation
- **Regression adjustment**  
  - Control for confounders statistically

#### Gold standard (where feasible)
- **Randomised Controlled Trials (RCTs)**  
  - Often possible for oversubscribed services, phased rollouts, or pilots

**Example (local government)**
> Evaluating a school attendance mentoring programme:  
> - Compare mentored pupils to similar pupils in neighbouring schools  
> - Use DiD to compare attendance change over two terms  
> - Adjust for baseline attendance and FSM eligibility  
> - Pre-register analysis plan before accessing outcome data  

---

### 3. Value for Money (VfM) evaluation  
**Purpose:** Assess whether outcomes justify the costs.  
**Best when:** Making commissioning decisions, defending budgets, comparing options.

VfM should be built on top of **credible impact evidence** where possible.

**Common approaches**
- **Cost-consequence analysis**  
  - Present costs alongside multiple outcomes (transparent but non-aggregated)
- **Cost-effectiveness analysis**  
  - e.g. £ per additional sustained tenancy
- **Cost-benefit analysis (CBA)**  
  - Monetise outcomes using sources such as:
    - HM Treasury Green Book values  
    - Unit costs (PSSRU, Home Office, DfE, NHSE, etc.)  
    - Local cost data where available
- **Sensitivity analysis**  
  - Show how results change under optimistic / pessimistic assumptions

**Example (local government)**
> Evaluating a rough sleeping prevention service:  
> - Estimate impact on avoided temporary accommodation placements  
> - Apply unit cost of TA per household-night  
> - Compare to programme delivery cost  
> - Model best-case and worst-case scenarios  

---

## Choosing the right method

| Your question | Likely approach |
|---------------|------------------|
| Is the service being delivered properly? | Process evaluation |
| Is this making a measurable difference? | Impact evaluation |
| Is this worth the investment? | Impact + VfM |
| Which of two models works better? | Comparison group / RCT |
| Can we scale this confidently? | Strong impact design + implementation evidence |

---

## Minimum viable evaluation plan (1–2 pages)

Every project should have at least this level of design clarity.

**1. Clear evaluation questions (usually 3–5)**
- One process question  
- One outcome/impact question  
- One decision-focused question  

**2. Outcomes and measures**
- Define primary outcome(s)  
- Define secondary outcomes  
- Specify data owners (e.g. Council, NHS, school, provider)  

**3. Design choice**
- Process only?  
- Before/after?  
- Comparator available?  
- Phased rollout that could enable randomisation?  

**4. Data, ethics and governance**
- DPIA completed  
- Information sharing agreements in place  
- Consent approach documented (where needed)  

**5. Analysis plan (proportionate but explicit)**
- What will be compared to what?  
- What statistical or qualitative approach will be used?  
- What would count as meaningful change?  

**6. Decision points**
- When will findings be reviewed?  
- What decisions will they inform (continue, stop, scale, redesign)?  

---

## A note on proportionality

Not every project needs an RCT.  
But every project should be **clear about its ambition and honest about its limitations**.

> Weak design + strong claims = loss of credibility  
> Proportionate design + transparency = stronger trust and better decisions
